{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Exporting Final Model\n",
    "\n",
    "Now that we have chosen a good final model from the ones we trained and tuned, it's time to export and save them. SuperGradients allows models to be saves as ONNX models that are easy to deploy and run inferences with, and that is what we do here.\n",
    "\n",
    "To condense the exporting notebook, I have omitted explanations to the steps which are already explained in the `03_supergradients_model.ipynb` notebook. Please refer back to it for further details.\n",
    "\n",
    "***NOTE! This notebook and model training was all run on Kaggle and/or SaturnCloud with heavy GPU augmentation, and took up to two hours to run. I would recommend loading the notebooks onto SaturnCloud to run them.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You did not mention an AWS environment.You can set the environment variable ENVIRONMENT_NAME with one of the values: development,staging,production\n",
      "deci_platform_sg_logger -WARNING- Failed to import deci_lab_client\n",
      "callbacks -WARNING- Failed to import deci_lab_client\n",
      "quantization_utils -WARNING- Failed to import pytorch_quantization\n",
      "env_sanity_check -INFO- ** A sanity check is done when importing super_gradients for the first time. **\n",
      "-> You can see the details by setting the env variable DISPLAY_SANITY_CHECK=True prior to import.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import super_gradients\n",
    "from pathlib import Path, PurePath\n",
    "\n",
    "from PIL import Image\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision import transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from super_gradients import init_trainer, Trainer\n",
    "from super_gradients.common import MultiGPUMode\n",
    "from super_gradients.training.utils.distributed_training_utils import setup_gpu_mode\n",
    "from super_gradients.training import Trainer\n",
    "from super_gradients.training import training_hyperparams\n",
    "\n",
    "from super_gradients.training import models\n",
    "from super_gradients import Trainer\n",
    "\n",
    "import bentoml\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    EXPERIMENT_NAME = 'kitchenware_classification'\n",
    "    MODEL_NAME = 'vit_large'\n",
    "    CHECKPOINT_DIR = 'checkpoints'\n",
    "    WEIGHTS = \"imagenet\"\n",
    "    TRAINING_PARAMS = \"training_hyperparams/imagenet_vit_train_params\"\n",
    "    NUM_CLASSES = 6\n",
    "    BATCH_SIZE = 16\n",
    "\n",
    "    # specify the paths to training and validation set \n",
    "    IMAGE_PATH = './data/images'\n",
    "    TRAIN_DATA = './data/train.csv'\n",
    "    TEST_DATA = './data/test.csv'\n",
    "\n",
    "    \n",
    "\n",
    "    # set the input height and width\n",
    "    INPUT_HEIGHT = 224\n",
    "    INPUT_WIDTH = 224\n",
    "\n",
    "    # set the input height and width\n",
    "    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "    IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "    NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(experiment_name=config.EXPERIMENT_NAME, ckpt_root_dir=config.CHECKPOINT_DIR)\n",
    "\n",
    "model = models.get(model_name=config.MODEL_NAME, num_classes=config.NUM_CLASSES, pretrained_weights=config.WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/super_gradients/training/training_hyperparams/training_hyperparams.py:24: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize_config_dir(config_dir=pkg_resources.resource_filename(\"super_gradients.recipes\", \"\")):\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'training_hyperparams/imagenet_vit_train_params': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "training_params =  training_hyperparams.get(config.TRAINING_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params[\"max_epochs\"] = 5\n",
    "training_params[\"zero_weight_decay_on_bias_and_bn\"] = True\n",
    "training_params['train_metrics_list'] = ['Accuracy']\n",
    "training_params['valid_metrics_list'] = ['Accuracy']\n",
    "training_params['ema'] = True\n",
    "training_params[\"criterion_params\"] = {'smooth_eps': 0.1} \n",
    "training_params['average_best_models'] = True\n",
    "training_params[\"sg_logger_params\"][\"launch_tensorboard\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5003, 556, 3808)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_image_col(df):\n",
    "    df['image'] = df['Id'].apply(lambda x: x +'.jpg')\n",
    "    \n",
    "# read labels into pandas df all cols as string\n",
    "labels_df = pd.read_csv(config.TRAIN_DATA, dtype='str')\n",
    "test_df = pd.read_csv(config.TEST_DATA, dtype='str')\n",
    "\n",
    "# create col (xxxx.jpg), the image filename\n",
    "add_image_col(labels_df)\n",
    "add_image_col(test_df)\n",
    "\n",
    "# map labels to integer\n",
    "le = LabelEncoder()\n",
    "labels_df['targets'] = le.fit_transform(labels_df['label'])\n",
    "\n",
    "#split into train and validation sets\n",
    "train_df, val_df = train_test_split(labels_df,  stratify= labels_df['targets'], test_size=.10, shuffle=True, random_state=42)\n",
    "\n",
    "# number of samples in each\n",
    "train_df.shape[0] , val_df.shape[0], test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KitchwareDataset(Dataset):\n",
    "    def __init__(self, dataframe , img_dir, split, transform = None):\n",
    "        self.img_labels = dataframe \n",
    "        self.img_dir = img_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "    \n",
    "    def __getitem__(self , idx):\n",
    "        if self.split in ['train', 'val']:\n",
    "            img_path = os.path.join(self.img_dir , self.img_labels.iloc[idx, 2])\n",
    "            label = self.img_labels.iloc[idx, 3]\n",
    "        else:\n",
    "            img_path = os.path.join(self.img_dir , self.img_labels.iloc[idx, 1])\n",
    "            \n",
    "        original_image = Image.open(img_path)\n",
    "        image = np.array(original_image)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "            \n",
    "        if self.split in ['train', 'val']: \n",
    "            return image, label \n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our data augmentation functions\n",
    "make_tensor = ToTensorV2()\n",
    "\n",
    "normalize = A.Normalize(mean=config.IMAGENET_MEAN, \n",
    "                        std=config.IMAGENET_STD)\n",
    "\n",
    "resize = A.Resize(height=config.INPUT_HEIGHT,\n",
    "                  width=config.INPUT_WIDTH)\n",
    "\n",
    "horizontal_flip = A.HorizontalFlip(p=0.50)\n",
    "\n",
    "flip = A.Flip(p=0.50)\n",
    "\n",
    "random_ninety = A.RandomRotate90()\n",
    "\n",
    "random_crop = A.RandomCrop(height=config.INPUT_HEIGHT,\n",
    "                           width=config.INPUT_WIDTH,\n",
    "                           p=0.75)\n",
    "\n",
    "hue_saturation = A.HueSaturationValue(p=.5)\n",
    "\n",
    "iso_noise = A.ISONoise(p=.5)\n",
    "\n",
    "color_jitter = A.ColorJitter(p=.5)\n",
    "\n",
    "emboss = A.Emboss(p=.5)\n",
    "\n",
    "channel_shuffle = A.ChannelShuffle(p=.5)\n",
    "\n",
    "randomly_choose_one = A.OneOf([flip, \n",
    "                               random_ninety, \n",
    "                               iso_noise,\n",
    "                               color_jitter,\n",
    "                               emboss,\n",
    "                               hue_saturation,\n",
    "                               channel_shuffle], p=.50)\n",
    "\n",
    "# initialize our training and validation set data augmentation pipeline\n",
    "train_transforms = A.Compose([\n",
    "  resize, \n",
    "  horizontal_flip, \n",
    "  random_crop,\n",
    "  randomly_choose_one,\n",
    "  normalize,\n",
    "  make_tensor\n",
    "])\n",
    "\n",
    "val_transforms = A.Compose([resize, normalize, make_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = KitchwareDataset(train_df , config.IMAGE_PATH , 'train', transform = train_transforms)\n",
    "val_data = KitchwareDataset(val_df , config.IMAGE_PATH , 'val', transform = val_transforms)\n",
    "test_data = KitchwareDataset(test_df, config.IMAGE_PATH, 'test',transform = val_transforms)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size = config.BATCH_SIZE , shuffle = True)\n",
    "val_dataloader = DataLoader(val_data, batch_size = config.BATCH_SIZE, shuffle = True)\n",
    "test_dataloader = DataLoader(test_data, batch_size = config.BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sg_trainer -INFO- Using EMA with params {'decay': 0.9999, 'beta': 15, 'exp_activation': True}\n",
      "\"events.out.tfevents.1671025833.w-aaish-kitchenware-classifica-972fef4f0a844bf6b8722c2a82avvgsx.1904.1\" will not be deleted\n",
      "\"events.out.tfevents.1671026630.w-aaish-kitchenware-classifica-972fef4f0a844bf6b8722c2a82asqvbk.124.0\" will not be deleted\n",
      "\"events.out.tfevents.1671025887.w-aaish-kitchenware-classifica-972fef4f0a844bf6b8722c2a82avvgsx.1904.2\" will not be deleted\n",
      "\"events.out.tfevents.1671024953.w-aaish-kitchenware-classifica-972fef4f0a844bf6b8722c2a82avvgsx.1904.0\" will not be deleted\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "optimizer_utils -WARNING- Module class: <class 'torch.nn.modules.normalization.LayerNorm'>, have a `bias` parameter attribute but is not instance of torch primitive modules, this bias parameter will be part of param group with zero weight decay.\n",
      "sg_trainer -INFO- Started training for 5 epochs (0/4)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 0:   0%|          | 0/313 [00:00<?, ?it/s]/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/torch/cuda/memory.py:384: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
      "  warnings.warn(\n",
      "Train epoch 0: 100%|██████████| 313/313 [09:59<00:00,  1.91s/it, Accuracy=0.957, LabelSmoothingCrossEntropyLoss=0.53, gpu_mem=11.3] \n",
      "Validation epoch 0: 100%|██████████| 35/35 [00:26<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================\n",
      "SUMMARY OF EPOCH 0\n",
      "├── Training\n",
      "│   └── Labelsmoothingcrossentropyloss = 0.5297\n",
      "└── Validation\n",
      "    ├── Accuracy = 0.9838\n",
      "    └── Labelsmoothingcrossentropyloss = 0.4708\n",
      "\n",
      "===========================================================\n",
      "base_sg_logger -INFO- Checkpoint saved in checkpoints/kitchenware_classification/ckpt_best.pth\n",
      "sg_trainer -INFO- Best checkpoint overriden: validation Accuracy: 0.9838129281997681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 1: 100%|██████████| 313/313 [10:05<00:00,  1.94s/it, Accuracy=0.985, LabelSmoothingCrossEntropyLoss=0.464, gpu_mem=11.3]\n",
      "Validation epoch 1: 100%|██████████| 35/35 [00:26<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================\n",
      "SUMMARY OF EPOCH 1\n",
      "├── Training\n",
      "│   └── Labelsmoothingcrossentropyloss = 0.4645\n",
      "│       ├── Best until now = 0.5297 (\u001b[32m↘ -0.0652\u001b[0m)\n",
      "│       └── Epoch N-1      = 0.5297 (\u001b[32m↘ -0.0652\u001b[0m)\n",
      "└── Validation\n",
      "    ├── Accuracy = 0.982\n",
      "    │   ├── Best until now = 0.9838 (\u001b[31m↘ -0.0018\u001b[0m)\n",
      "    │   └── Epoch N-1      = 0.9838 (\u001b[31m↘ -0.0018\u001b[0m)\n",
      "    └── Labelsmoothingcrossentropyloss = 0.4699\n",
      "        ├── Best until now = 0.4708 (\u001b[32m↘ -0.0009\u001b[0m)\n",
      "        └── Epoch N-1      = 0.4708 (\u001b[32m↘ -0.0009\u001b[0m)\n",
      "\n",
      "===========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 2: 100%|██████████| 313/313 [10:02<00:00,  1.92s/it, Accuracy=0.991, LabelSmoothingCrossEntropyLoss=0.446, gpu_mem=11.3]\n",
      "Validation epoch 2: 100%|██████████| 35/35 [00:26<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================\n",
      "SUMMARY OF EPOCH 2\n",
      "├── Training\n",
      "│   └── Labelsmoothingcrossentropyloss = 0.446\n",
      "│       ├── Best until now = 0.4645 (\u001b[32m↘ -0.0185\u001b[0m)\n",
      "│       └── Epoch N-1      = 0.4645 (\u001b[32m↘ -0.0185\u001b[0m)\n",
      "└── Validation\n",
      "    ├── Accuracy = 0.9838\n",
      "    │   ├── Best until now = 0.9838 (\u001b[32m↘ 0.0\u001b[0m)\n",
      "    │   └── Epoch N-1      = 0.982  (\u001b[32m↗ 0.0018\u001b[0m)\n",
      "    └── Labelsmoothingcrossentropyloss = 0.4665\n",
      "        ├── Best until now = 0.4699 (\u001b[32m↘ -0.0034\u001b[0m)\n",
      "        └── Epoch N-1      = 0.4699 (\u001b[32m↘ -0.0034\u001b[0m)\n",
      "\n",
      "===========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 3: 100%|██████████| 313/313 [10:00<00:00,  1.92s/it, Accuracy=0.996, LabelSmoothingCrossEntropyLoss=0.433, gpu_mem=11.3]\n",
      "Validation epoch 3: 100%|██████████| 35/35 [00:25<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================\n",
      "SUMMARY OF EPOCH 3\n",
      "├── Training\n",
      "│   └── Labelsmoothingcrossentropyloss = 0.4326\n",
      "│       ├── Best until now = 0.446  (\u001b[32m↘ -0.0134\u001b[0m)\n",
      "│       └── Epoch N-1      = 0.446  (\u001b[32m↘ -0.0134\u001b[0m)\n",
      "└── Validation\n",
      "    ├── Accuracy = 0.9838\n",
      "    │   ├── Best until now = 0.9838 (\u001b[32m↘ 0.0\u001b[0m)\n",
      "    │   └── Epoch N-1      = 0.9838 (\u001b[32m↘ 0.0\u001b[0m)\n",
      "    └── Labelsmoothingcrossentropyloss = 0.466\n",
      "        ├── Best until now = 0.4665 (\u001b[32m↘ -0.0005\u001b[0m)\n",
      "        └── Epoch N-1      = 0.4665 (\u001b[32m↘ -0.0005\u001b[0m)\n",
      "\n",
      "===========================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 4: 100%|██████████| 313/313 [09:58<00:00,  1.91s/it, Accuracy=0.997, LabelSmoothingCrossEntropyLoss=0.428, gpu_mem=11.3]\n",
      "Validation epoch 4: 100%|██████████| 35/35 [00:25<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================\n",
      "SUMMARY OF EPOCH 4\n",
      "├── Training\n",
      "│   └── Labelsmoothingcrossentropyloss = 0.4284\n",
      "│       ├── Best until now = 0.4326 (\u001b[32m↘ -0.0042\u001b[0m)\n",
      "│       └── Epoch N-1      = 0.4326 (\u001b[32m↘ -0.0042\u001b[0m)\n",
      "└── Validation\n",
      "    ├── Accuracy = 0.9838\n",
      "    │   ├── Best until now = 0.9838 (\u001b[32m↘ 0.0\u001b[0m)\n",
      "    │   └── Epoch N-1      = 0.9838 (\u001b[32m↘ 0.0\u001b[0m)\n",
      "    └── Labelsmoothingcrossentropyloss = 0.4657\n",
      "        ├── Best until now = 0.466  (\u001b[32m↘ -0.0003\u001b[0m)\n",
      "        └── Epoch N-1      = 0.466  (\u001b[32m↘ -0.0003\u001b[0m)\n",
      "\n",
      "===========================================================\n",
      "sg_trainer -INFO- RUNNING ADDITIONAL TEST ON THE AVERAGED MODEL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation epoch 5: 100%|██████████| 35/35 [00:24<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================\n",
      "SUMMARY OF EPOCH 5\n",
      "├── Training\n",
      "│   └── Labelsmoothingcrossentropyloss = 0.4284\n",
      "│       ├── Best until now = 0.4326 (\u001b[32m↘ -0.0042\u001b[0m)\n",
      "│       └── Epoch N-1      = 0.4326 (\u001b[32m↘ -0.0042\u001b[0m)\n",
      "└── Validation\n",
      "    ├── Accuracy = 0.9856\n",
      "    │   ├── Best until now = 0.9838 (\u001b[32m↗ 0.0018\u001b[0m)\n",
      "    │   └── Epoch N-1      = 0.9838 (\u001b[32m↗ 0.0018\u001b[0m)\n",
      "    └── Labelsmoothingcrossentropyloss = 0.465\n",
      "        ├── Best until now = 0.4657 (\u001b[32m↘ -0.0007\u001b[0m)\n",
      "        └── Epoch N-1      = 0.4657 (\u001b[32m↘ -0.0007\u001b[0m)\n",
      "\n",
      "===========================================================\n"
     ]
    }
   ],
   "source": [
    "trainer.train(model=model, \n",
    "              training_params=training_params, \n",
    "              train_loader=train_dataloader,\n",
    "              valid_loader=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model that we trained\n",
    "best_model = models.get(config.MODEL_NAME,\n",
    "                        num_classes=config.NUM_CLASSES,\n",
    "                        checkpoint_path=os.path.join(trainer.checkpoints_dir_path, \"ckpt_best.pth\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model\n",
    "\n",
    "This is the step that saves the best model that we loaded previously as an ONNX model. This will save a seperate file into the directory which can be accessed by the other files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "best_model.prep_model_for_conversion(input_size=[1, 3, 224, 224])\n",
    "dummy_input = torch.randn([1, 3, 224, 224], device=next(best_model.parameters()).device)\n",
    "\n",
    "torch.onnx.export(\n",
    "    best_model, \n",
    "    dummy_input, \n",
    "    \"kitchenware_model.onnx\",\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlzoomcamp_capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "78ae5a3a902558c302b80cf107cf0cca8f368010a0c8f015ef724d2b4beb1d2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
